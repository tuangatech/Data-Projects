{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # NYC Yellow Taxi Data Processing Pipeline\n",
    "# MAGIC \n",
    "# MAGIC This notebook processes raw NYC yellow taxi data and creates features for fare prediction.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Setup and Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get parameters from job\n",
    "source_year = dbutils.widgets.get(\"source_year\")\n",
    "source_month = dbutils.widgets.get(\"source_month\")\n",
    "source_bucket = dbutils.widgets.get(\"source_bucket\")\n",
    "source_key = dbutils.widgets.get(\"source_key\")\n",
    "\n",
    "print(f\"Processing: {source_bucket}/{source_key}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## CloudWatch Metrics Setup\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# To properly send CloudWatch metrics, update your Databricks job configuration to include an instance profile with CloudWatch permissions\n",
    "# def send_cloudwatch_metric(metric_name, value, unit='Count', dimensions=None):\n",
    "#     \"\"\"Send custom metrics to CloudWatch\"\"\"\n",
    "#     try:\n",
    "#         cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')\n",
    "        \n",
    "#         metric_data = {\n",
    "#             'MetricName': metric_name,\n",
    "#             'Value': value,\n",
    "#             'Unit': unit,\n",
    "#             'Timestamp': datetime.utcnow()\n",
    "#         }\n",
    "        \n",
    "#         if dimensions:\n",
    "#             metric_data['Dimensions'] = dimensions\n",
    "        \n",
    "#         cloudwatch.put_metric_data(\n",
    "#             Namespace='NYCTaxi/Processing',\n",
    "#             MetricData=[metric_data]\n",
    "#         )\n",
    "#         logger.info(f\"Sent metric: {metric_name} = {value}\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Failed to send CloudWatch metric: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Ingestion and Initial Validation\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_and_validate_data(file_path):\n",
    "    \"\"\"Load raw data and perform initial validation\"\"\"\n",
    "    try:\n",
    "        # Load data\n",
    "        df = spark.read.parquet(f\"s3a://{source_bucket}/{source_key}\")\n",
    "        \n",
    "        initial_count = df.count()\n",
    "        logger.info(f\"METRIC: Initial data load: {initial_count} records from {file_path}\")\n",
    "        \n",
    "        # Send metric\n",
    "        # send_cloudwatch_metric(\n",
    "        #     'RecordsLoaded', \n",
    "        #     initial_count,\n",
    "        #     dimensions=[\n",
    "        #         {'Name': 'Year', 'Value': source_year},\n",
    "        #         {'Name': 'Month', 'Value': source_month}\n",
    "        #     ]\n",
    "        # )\n",
    "        \n",
    "        return df, initial_count\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data: {e}\")\n",
    "        # send_cloudwatch_metric('DataLoadFailed', 1)\n",
    "        raise Exception(f\"Failed to load data: {e}\")\n",
    "\n",
    "# Load the data\n",
    "raw_df, initial_record_count = load_and_validate_data(f\"{source_bucket}/{source_key}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Quality Checks and Cleaning\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Apply data quality checks and cleaning rules\"\"\"\n",
    "    \n",
    "    # Initial count\n",
    "    start_count = df.count()\n",
    "    \n",
    "    # Remove records with invalid coordinates\n",
    "    df_clean = df.filter(\n",
    "        (F.col(\"PULocationID\").isNotNull()) &\n",
    "        (F.col(\"DOLocationID\").isNotNull()) &\n",
    "        (F.col(\"trip_distance\") > 0) &\n",
    "        (F.col(\"trip_distance\") < 1000) &  # Remove outliers\n",
    "        (F.col(\"fare_amount\") > 0) &\n",
    "        (F.col(\"fare_amount\") < 1000) &    # Remove outliers\n",
    "        (F.col(\"total_amount\") > 0) &\n",
    "        (F.col(\"passenger_count\") > 0) &\n",
    "        (F.col(\"passenger_count\") <= 6) &  # Reasonable passenger count\n",
    "        (F.col(\"tpep_pickup_datetime\").isNotNull()) &\n",
    "        (F.col(\"tpep_dropoff_datetime\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    # Remove trips with negative duration\n",
    "    df_clean = df_clean.filter(\n",
    "        F.col(\"tpep_dropoff_datetime\") > F.col(\"tpep_pickup_datetime\")\n",
    "    )\n",
    "    \n",
    "    # Remove trips longer than 24 hours (likely data errors)\n",
    "    df_clean = df_clean.filter(\n",
    "        (F.unix_timestamp(\"tpep_dropoff_datetime\") - \n",
    "         F.unix_timestamp(\"tpep_pickup_datetime\")) < 86400\n",
    "    )\n",
    "    \n",
    "    final_count = df_clean.count()\n",
    "    removed_count = start_count - final_count\n",
    "    \n",
    "    logger.info(f\"Data cleaning: {start_count} -> {final_count} records (removed {removed_count})\")\n",
    "    \n",
    "    # Send quality metrics\n",
    "    logger.info(f'METRIC: RecordsRemoved {removed_count}')\n",
    "    logger.info(f'METRIC: DataQualityScore {(final_count / start_count) * 100} Percent')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "cleaned_df = clean_data(raw_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Feature Engineering\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create features for fare prediction model\"\"\"\n",
    "    \n",
    "    logger.info(\"Starting feature engineering...\")\n",
    "    \n",
    "    # Calculate trip duration in minutes\n",
    "    df_features = df.withColumn(\n",
    "        \"duration_minutes\",\n",
    "        (F.unix_timestamp(F.col(\"tpep_dropoff_datetime\")) - \n",
    "        F.unix_timestamp(F.col(\"tpep_pickup_datetime\"))) / 60\n",
    "    )\n",
    "    \n",
    "    # Temporal features\n",
    "    df_features = df_features.withColumn(\"pickup_hour\", F.hour(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"pickup_day_of_week\", F.dayofweek(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"pickup_year\", F.year(\"tpep_pickup_datetime\"))\n",
    "    \n",
    "    # Derived temporal features\n",
    "    df_features = df_features \\\n",
    "        .withColumn(\"is_weekend\", F.when(F.col(\"pickup_day_of_week\").isin([1, 7]), 1).otherwise(0)) \\\n",
    "        .withColumn(\"is_rush_hour\", \n",
    "                   F.when((F.col(\"pickup_hour\").between(7, 9)) | \n",
    "                         (F.col(\"pickup_hour\").between(17, 19)), 1).otherwise(0)) \\\n",
    "        .withColumn(\"season\", \n",
    "                   F.when(F.col(\"pickup_month\").isin([12, 1, 2]), \"winter\")\n",
    "                    .when(F.col(\"pickup_month\").isin([3, 4, 5]), \"spring\")\n",
    "                    .when(F.col(\"pickup_month\").isin([6, 7, 8]), \"summer\")\n",
    "                    .otherwise(\"fall\"))\n",
    "    \n",
    "    # Trip characteristics\n",
    "    df_features = df_features \\\n",
    "        .withColumn(\"trip_distance_km\", F.col(\"trip_distance\") * 1.60934) \\\n",
    "        .withColumn(\"avg_speed_kmh\", \n",
    "                   F.when(F.col(\"duration_minutes\") > 0,\n",
    "                         (F.col(\"trip_distance_km\") / F.col(\"duration_minutes\")) * 60)\n",
    "                    .otherwise(0)) \\\n",
    "        .withColumn(\"fare_per_mile\", F.col(\"fare_amount\") / F.col(\"trip_distance\")) \\\n",
    "        .withColumn(\"fare_per_minute\", F.col(\"fare_amount\") / F.col(\"duration_minutes\"))\n",
    "    \n",
    "    # Payment features\n",
    "    df_features = df_features \\\n",
    "        .withColumn(\"tip_percentage\", \n",
    "                   F.when(F.col(\"fare_amount\") > 0,\n",
    "                         (F.col(\"tip_amount\") / F.col(\"fare_amount\")) * 100)\n",
    "                    .otherwise(0)) \\\n",
    "        .withColumn(\"total_amount_per_passenger\", \n",
    "                   F.col(\"total_amount\") / F.col(\"passenger_count\"))\n",
    "    \n",
    "    # Location features (simplified borough mapping)\n",
    "    # Manhattan zones: 1-4, 12-13, 24-25, 41-43, 45, 48, 50, 68, 74-75, 79, 87-88, 90, 100, 103-104, 107-108, 113, 114, 116, 120-125, 127-128, 137, 140, 141-144, 148, 151-153, 158, 161-163, 164, 166, 170, 186, 194, 202, 209, 211, 224, 229-234, 236-237, 238, 239, 243, 244, 246, 249-250, 261-263\n",
    "    manhattan_zones = [1,2,3,4,12,13,24,25,41,42,43,45,48,50,68,74,75,79,87,88,90,100,103,104,107,108,113,114,116,120,121,122,123,124,125,127,128,137,140,141,142,143,144,148,151,152,153,158,161,162,163,164,166,170,186,194,202,209,211,224,229,230,231,232,233,234,236,237,238,239,243,244,246,249,250,261,262,263]\n",
    "    \n",
    "    # Airport zones (JFK: 132, LGA: 138, Newark: 1)  \n",
    "    airport_zones = [132, 138, 1]\n",
    "    \n",
    "    df_features = df_features \\\n",
    "        .withColumn(\"pickup_manhattan\", \n",
    "                   F.when(F.col(\"PULocationID\").isin(manhattan_zones), 1).otherwise(0)) \\\n",
    "        .withColumn(\"dropoff_manhattan\", \n",
    "                   F.when(F.col(\"DOLocationID\").isin(manhattan_zones), 1).otherwise(0)) \\\n",
    "        .withColumn(\"manhattan_trip\",\n",
    "                   F.when((F.col(\"pickup_manhattan\") == 1) | (F.col(\"dropoff_manhattan\") == 1), 1).otherwise(0)) \\\n",
    "        .withColumn(\"is_airport_trip\",\n",
    "                   F.when(F.col(\"PULocationID\").isin(airport_zones) | \n",
    "                         F.col(\"DOLocationID\").isin(airport_zones), 1).otherwise(0))\n",
    "    \n",
    "    # Add processing metadata\n",
    "    df_features = df_features \\\n",
    "        .withColumn(\"processed_timestamp\", F.current_timestamp()) \\\n",
    "        .withColumn(\"processing_year\", F.lit(source_year)) \\\n",
    "        .withColumn(\"processing_month\", F.lit(source_month))\n",
    "    \n",
    "    logger.info(\"Feature engineering completed\")\n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "feature_df = engineer_features(cleaned_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Final Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def validate_features(df):\n",
    "    \"\"\"Validate engineered features\"\"\"\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # Check for null values in key features\n",
    "    key_features = [\"duration_minutes\", \"avg_speed_kmh\", \"fare_per_mile\", \"tip_percentage\"]\n",
    "    \n",
    "    for feature in key_features:\n",
    "        null_count = df.filter(F.col(feature).isNull()).count()\n",
    "        validation_results[f\"{feature}_nulls\"] = null_count\n",
    "        \n",
    "        if null_count > 0:\n",
    "            logger.warning(f\"Found {null_count} null values in {feature}\")\n",
    "    \n",
    "    # Check for unreasonable values\n",
    "    unreasonable_speed = df.filter(F.col(\"avg_speed_kmh\") > 200).count()\n",
    "    unreasonable_duration = df.filter(F.col(\"duration_minutes\") > 1440).count()  # > 24 hours\n",
    "    \n",
    "    validation_results[\"unreasonable_speed_count\"] = unreasonable_speed\n",
    "    validation_results[\"unreasonable_duration_count\"] = unreasonable_duration\n",
    "    \n",
    "    # Send validation metrics\n",
    "    for metric_name, value in validation_results.items():\n",
    "        logger.info(f\"METRIC: Validation_{metric_name} = {value}\")\n",
    "    \n",
    "    logger.info(f\"Feature validation completed: {validation_results}\")\n",
    "    return validation_results\n",
    "\n",
    "validation_results = validate_features(feature_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Save to Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def setup_unity_catalog():\n",
    "    \"\"\"Create catalog and schema if they don't exist\"\"\"\n",
    "    try:\n",
    "        # Create catalog with managed location\n",
    "        catalog_location = f\"s3a://{source_bucket}/unity-catalog/nyc_taxi_analytics/\"\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE CATALOG IF NOT EXISTS nyc_taxi_analytics\n",
    "            MANAGED LOCATION '{catalog_location}'\n",
    "        \"\"\")\n",
    "        logger.info(\"Catalog 'nyc_taxi_analytics' created or already exists\")\n",
    "        \n",
    "        # Create schema\n",
    "        spark.sql(\"CREATE SCHEMA IF NOT EXISTS nyc_taxi_analytics.fare_prediction\")\n",
    "        logger.info(\"Schema 'nyc_taxi_analytics.fare_prediction' created or already exists\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to setup Unity Catalog: {e}\")\n",
    "        raise e\n",
    "\n",
    "setup_unity_catalog()\n",
    "\n",
    "\n",
    "def save_to_delta(df, table_path, table_name):\n",
    "    \"\"\"Save processed data to Delta Lake format\"\"\"\n",
    "    \n",
    "    try:\n",
    "        record_count = df.count()\n",
    "        logger.info(f\"Saving {record_count} records to {table_path}\")\n",
    "        \n",
    "        # Write to Delta Lake with partitioning\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .partitionBy(\"processing_year\", \"processing_month\") \\\n",
    "            .save(table_path)\n",
    "        \n",
    "        # Register/update table in Unity Catalog\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{table_path}'\n",
    "        \"\"\")\n",
    "        \n",
    "        # Send success metrics\n",
    "        logger.info(f\"METRIC: RecordsWritten {record_count}\")\n",
    "        # send_cloudwatch_metric('ProcessingSuccess', 1)\n",
    "        \n",
    "        logger.info(f\"Successfully saved data to {table_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        # send_cloudwatch_metric('ProcessingFailed', 1)\n",
    "        logger.error(f\"Failed to save data: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Define paths and table names\n",
    "processed_table_path = f\"s3a://{source_bucket}/nyctaxi/processed/yellow_taxi_features/\"\n",
    "catalog_table_name = \"nyc_taxi_analytics.fare_prediction.processed_yellow_taxi\"\n",
    "\n",
    "# Save the processed data\n",
    "save_success = save_to_delta(feature_df, processed_table_path, catalog_table_name)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Processing Summary and Cleanup\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def log_processing_summary():\n",
    "    \"\"\"Log final processing summary\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        \"source_file\": f\"{source_bucket}/{source_key}\",\n",
    "        \"processing_date\": datetime.utcnow().isoformat(),\n",
    "        \"initial_records\": initial_record_count,\n",
    "        \"final_records\": feature_df.count(),\n",
    "        \"success\": save_success\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Processing Summary: {summary}\")\n",
    "    \n",
    "    # Save processing log to S3\n",
    "    log_path = f\"s3a://{source_bucket}/nyctaxi/processing_logs/success/{source_year}_{source_month}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    log_df = spark.createDataFrame([summary])\n",
    "    log_df.coalesce(1).write.mode(\"overwrite\").json(log_path)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final summary\n",
    "processing_summary = log_processing_summary()\n",
    "\n",
    "print(\"âœ… Processing completed successfully!\")\n",
    "print(f\"ðŸ“Š Processed {processing_summary['final_records']} records\")\n",
    "print(f\"ðŸ’¾ Data saved to: {catalog_table_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
